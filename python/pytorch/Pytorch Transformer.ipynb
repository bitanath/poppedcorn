{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd467b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b44b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"nietzsche.txt\").read()\n",
    "chars = sorted(list(set(text)))\n",
    "chars.insert(0, '\\0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa0b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {v: i for i, v in enumerate(chars)}\n",
    "index_to_char = {i: v for i, v in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d13b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_index = [char_to_index[char] for char in text]\n",
    "pred_num = 25\n",
    "xin = [[total_index[j + i] for j in range(0, len(total_index) - 1 - pred_num, pred_num)] for i in range(pred_num)]\n",
    "y = [total_index[i + pred_num] for i in range(0, len(total_index) - 1 - pred_num, pred_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8d1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.stack(xin[i][:-2]) for i in range(pred_num)],1)\n",
    "Y = np.stack(y[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b376274",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a2e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__( self, in_size: int, out_size: int, n_heads: int, n_timesteps: int, dropout_prob: float = 0, device = 'cpu' ):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "        self.Wk = nn.Linear(in_size, in_size, bias=True)\n",
    "        self.Wq = nn.Linear(in_size, in_size, bias=True)\n",
    "        self.Wv = nn.Linear(in_size, in_size, bias=True)\n",
    "        self.residual_proj = nn.Linear(in_size, out_size, bias=True)\n",
    "        \n",
    "        # Create lower triangular mask\n",
    "        mask = torch.tril(torch.ones(n_timesteps, n_timesteps))\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "        self.att_dropout = nn.Dropout(dropout_prob)\n",
    "        self.residual_dropout = nn.Dropout(dropout_prob)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Store head_size and verify that it's an integer\n",
    "        self.H = in_size // n_heads\n",
    "        if in_size % n_heads != 0:\n",
    "            raise ValueError(\"Embedding dimension not divisible in equal heads.\")\n",
    "        \n",
    "        # Move to specified device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, D = x.shape\n",
    "        H = self.H\n",
    "        nh = D // H  # Num heads\n",
    "        \n",
    "        # Get key, queries and values from the input\n",
    "        k = self.Wk(x)  # (B, T, D)\n",
    "        q = self.Wq(x)  # (B, T, D)\n",
    "        v = self.Wv(x)  # (B, T, D)\n",
    "        \n",
    "        # Reshape into different heads\n",
    "        k = k.reshape(B, T, nh, H).transpose(1, 2)  # (B, nh, T, H)\n",
    "        q = q.reshape(B, T, nh, H).transpose(1, 2)  # (B, nh, T, H)\n",
    "        v = v.reshape(B, T, nh, H).transpose(1, 2)  # (B, nh, T, H)\n",
    "        \n",
    "        # Compute attention activation\n",
    "        kT = k.transpose(-2, -1)  # (B, nh, H, T)\n",
    "        att = torch.matmul(q, kT)  # (B, nh, T, T)\n",
    "        \n",
    "        # Scale attention scores\n",
    "        att = att / (H ** 2)\n",
    "        \n",
    "        # Apply mask (to block out future characters)\n",
    "        mask = self.mask[:T, :T]  # Get appropriate size mask\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n",
    "        att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax and dropout\n",
    "        att = self.softmax(att)\n",
    "        att = self.att_dropout(att)\n",
    "        \n",
    "        # Compute weighted sum between values\n",
    "        out = torch.matmul(att, v)  # (B, nh, T, H)\n",
    "        \n",
    "        # Restack heads in D dimension\n",
    "        out = out.transpose(1, 2).reshape(B, T, D)  # (B, T, D)\n",
    "        \n",
    "        # Apply final projection (Dense layer) and dropout\n",
    "        out = self.residual_proj(out)  # (B, T, out_size)\n",
    "        out = self.residual_dropout(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b27c6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, in_size: int, out_size: int, dropout_prob: float = 0, device: str = 'cpu', bias: bool = True):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(in_size, in_size * 2, bias=bias)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(in_size * 2, out_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Move to specified device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.l1(x)\n",
    "        z = self.relu(z)\n",
    "        z = self.l2(z)\n",
    "        z = self.dropout(z)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "118b2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__( self, in_size: int, out_size: int, n_heads: int, n_timesteps: int, dropout_prob: float = 0, device: str = 'cpu' ):\n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        self.att = MultiHeadSelfAttention( in_size, in_size, n_heads, n_timesteps, dropout_prob, device )\n",
    "        self.ln1 = nn.LayerNorm(in_size)\n",
    "        self.fcc = FullyConnected(in_size, out_size, dropout_prob, device, True)\n",
    "        self.ln2 = nn.LayerNorm(out_size)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = x + self.att(self.ln1(x))\n",
    "        z = z + self.fcc(self.ln2(z))\n",
    "        \n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9125b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, input_size: int, embed_size: int):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.E = nn.Parameter(torch.randn(input_size, embed_size))\n",
    "    \n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        _, T = idx.shape\n",
    "        \n",
    "        positions = torch.arange(T, device=idx.device) # Create indices for the positions (0 to T-1)\n",
    "        x = self.E[positions]\n",
    "        \n",
    "        batch_size = idx.shape[0]\n",
    "        x = x.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c5d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model translated from TypeScript\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, n_timesteps, n_heads, dropout_p=0.2, device=None):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_embed = PositionalEmbedding(n_timesteps, hidden_size)\n",
    "        self.b1 = Block(hidden_size, hidden_size, n_heads, n_timesteps, dropout_p, device)\n",
    "        self.b2 = Block(hidden_size, hidden_size, n_heads, n_timesteps, dropout_p, device)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        if device:\n",
    "            self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embedding and positional encoding\n",
    "        z = self.embed(x)\n",
    "        z = z + self.pos_embed(x)  # Simplified addition\n",
    "        \n",
    "        z = self.b1(z)\n",
    "        z = self.b2(z)\n",
    "        \n",
    "        z = self.ln(z)\n",
    "        last_token_output = z[:, -1, :]\n",
    "\n",
    "        out = self.linear(last_token_output)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07ce069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple transformer in pure Pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, hidden_dim):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder( nn.TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward=hidden_dim,dropout=0.5),num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "\n",
    "        transformer_out = self.transformer_encoder(embedded) \n",
    "        transformer_out = transformer_out.transpose(0, 1) \n",
    "        \n",
    "        last_token_output = transformer_out[:, -1, :]\n",
    "\n",
    "        out = self.fc(last_token_output) # (batch_size, vocab_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75df80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 86\n",
    "embedding_dim = 42\n",
    "num_heads = 2  # Number of attention heads\n",
    "num_layers = 2 # Number of Transformer encoder layers\n",
    "hidden_dim = 128 # Hidden dimension for feedforward network in Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eb0bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bitan/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SimpleTransformer(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim)\n",
    "# model = Transformer( vocab_size=vocab_size, hidden_size=hidden_dim, n_timesteps=25, n_heads=num_heads, dropout_p=0.2, device='cpu' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c9e19d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "SimpleTransformer                                                 --\n",
       "├─Embedding: 1-1                                                  3,612\n",
       "├─TransformerEncoder: 1-2                                         --\n",
       "│    └─ModuleList: 2-1                                            --\n",
       "│    │    └─TransformerEncoderLayer: 3-1                          18,314\n",
       "│    │    └─TransformerEncoderLayer: 3-2                          18,314\n",
       "├─Linear: 1-3                                                     3,698\n",
       "├─PositionalEncoding: 1-4                                         --\n",
       "│    └─Dropout: 2-2                                               --\n",
       "==========================================================================================\n",
       "Total params: 43,938\n",
       "Trainable params: 43,938\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "649eb22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 43938 Total: 43938\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable Parameters:\",sum(p.numel() for p in model.parameters() if p.requires_grad),\"Total:\",sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb017561",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73ac8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.7215, Time: 3.30s\n",
      "Epoch [2/100], Loss: 2.7262, Time: 3.16s\n",
      "Epoch [3/100], Loss: 2.6038, Time: 3.14s\n",
      "Epoch [4/100], Loss: 2.7319, Time: 3.20s\n",
      "Epoch [5/100], Loss: 2.5058, Time: 3.13s\n",
      "Epoch [6/100], Loss: 2.5415, Time: 3.14s\n",
      "Epoch [7/100], Loss: 2.4053, Time: 3.13s\n",
      "Epoch [8/100], Loss: 2.4882, Time: 3.21s\n",
      "Epoch [9/100], Loss: 2.5397, Time: 3.56s\n",
      "Epoch [10/100], Loss: 2.4738, Time: 3.36s\n",
      "Epoch [11/100], Loss: 2.5524, Time: 3.41s\n",
      "Epoch [12/100], Loss: 2.4964, Time: 3.09s\n",
      "Epoch [13/100], Loss: 2.4651, Time: 2.98s\n",
      "Epoch [14/100], Loss: 2.5220, Time: 2.99s\n",
      "Epoch [15/100], Loss: 2.5625, Time: 2.98s\n",
      "Epoch [16/100], Loss: 2.5308, Time: 3.01s\n",
      "Epoch [17/100], Loss: 2.5118, Time: 2.96s\n",
      "Epoch [18/100], Loss: 2.5505, Time: 2.99s\n",
      "Epoch [19/100], Loss: 2.4972, Time: 2.99s\n",
      "Epoch [20/100], Loss: 2.5287, Time: 3.00s\n",
      "Epoch [21/100], Loss: 2.5252, Time: 3.00s\n",
      "Epoch [22/100], Loss: 2.3908, Time: 2.98s\n",
      "Epoch [23/100], Loss: 2.4932, Time: 2.98s\n",
      "Epoch [24/100], Loss: 2.4242, Time: 2.98s\n",
      "Epoch [25/100], Loss: 2.4474, Time: 2.99s\n",
      "Epoch [26/100], Loss: 2.4630, Time: 3.01s\n",
      "Epoch [27/100], Loss: 2.4517, Time: 2.99s\n",
      "Epoch [28/100], Loss: 2.5044, Time: 2.99s\n",
      "Epoch [29/100], Loss: 2.4544, Time: 2.98s\n",
      "Epoch [30/100], Loss: 2.4227, Time: 3.00s\n",
      "Epoch [31/100], Loss: 2.5134, Time: 2.98s\n",
      "Epoch [32/100], Loss: 2.5294, Time: 2.95s\n",
      "Epoch [33/100], Loss: 2.4559, Time: 2.99s\n",
      "Epoch [34/100], Loss: 2.3777, Time: 2.97s\n",
      "Epoch [35/100], Loss: 2.4567, Time: 3.01s\n",
      "Epoch [36/100], Loss: 2.4731, Time: 3.05s\n",
      "Epoch [37/100], Loss: 2.5014, Time: 3.02s\n",
      "Epoch [38/100], Loss: 2.4603, Time: 2.98s\n",
      "Epoch [39/100], Loss: 2.5500, Time: 3.03s\n",
      "Epoch [40/100], Loss: 2.4365, Time: 2.97s\n",
      "Epoch [41/100], Loss: 2.4791, Time: 2.99s\n",
      "Epoch [42/100], Loss: 2.3923, Time: 2.99s\n",
      "Epoch [43/100], Loss: 2.4600, Time: 2.99s\n",
      "Epoch [44/100], Loss: 2.3686, Time: 2.99s\n",
      "Epoch [45/100], Loss: 2.4577, Time: 2.99s\n",
      "Epoch [46/100], Loss: 2.3933, Time: 2.99s\n",
      "Epoch [47/100], Loss: 2.4022, Time: 2.99s\n",
      "Epoch [48/100], Loss: 2.4812, Time: 2.99s\n",
      "Epoch [49/100], Loss: 2.4500, Time: 2.97s\n",
      "Epoch [50/100], Loss: 2.3910, Time: 2.99s\n",
      "Epoch [51/100], Loss: 2.4840, Time: 3.00s\n",
      "Epoch [52/100], Loss: 2.3707, Time: 2.95s\n",
      "Epoch [53/100], Loss: 2.4470, Time: 2.99s\n",
      "Epoch [54/100], Loss: 2.3934, Time: 2.99s\n",
      "Epoch [55/100], Loss: 2.4255, Time: 2.97s\n",
      "Epoch [56/100], Loss: 2.4190, Time: 2.99s\n",
      "Epoch [57/100], Loss: 2.4338, Time: 2.99s\n",
      "Epoch [58/100], Loss: 2.4022, Time: 2.99s\n",
      "Epoch [59/100], Loss: 2.4224, Time: 2.97s\n",
      "Epoch [60/100], Loss: 2.3636, Time: 2.98s\n",
      "Epoch [61/100], Loss: 2.4480, Time: 2.99s\n",
      "Epoch [62/100], Loss: 2.4290, Time: 2.99s\n",
      "Epoch [63/100], Loss: 2.3908, Time: 2.97s\n",
      "Epoch [64/100], Loss: 2.4691, Time: 3.00s\n",
      "Epoch [65/100], Loss: 2.4190, Time: 3.00s\n",
      "Epoch [66/100], Loss: 2.3752, Time: 2.99s\n",
      "Epoch [67/100], Loss: 2.4270, Time: 2.99s\n",
      "Epoch [68/100], Loss: 2.4831, Time: 2.98s\n",
      "Epoch [69/100], Loss: 2.3964, Time: 2.98s\n",
      "Epoch [70/100], Loss: 2.4867, Time: 2.96s\n",
      "Epoch [71/100], Loss: 2.2879, Time: 3.00s\n",
      "Epoch [72/100], Loss: 2.3742, Time: 2.99s\n",
      "Epoch [73/100], Loss: 2.4127, Time: 2.96s\n",
      "Epoch [74/100], Loss: 2.4152, Time: 2.99s\n",
      "Epoch [75/100], Loss: 2.3371, Time: 2.97s\n",
      "Epoch [76/100], Loss: 2.3792, Time: 3.00s\n",
      "Epoch [77/100], Loss: 2.3697, Time: 2.99s\n",
      "Epoch [78/100], Loss: 2.4081, Time: 3.04s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    current_time = time.time()\n",
    "    for i in range(0, len(X), batch_size):\n",
    "\n",
    "        X_batch = X_tensor[i:i + batch_size]\n",
    "        Y_batch = Y_tensor[i:i + batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}, Time: {time.time() - current_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(inp):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    index = [char_to_index[i] for i in inp]\n",
    "    arr = np.expand_dims(np.array(index), axis=0)\n",
    "    input_tensor = torch.tensor(arr, dtype=torch.long) # Convert to tensor\n",
    "    with torch.no_grad(): # Disable gradient calculation during inference\n",
    "        prediction = model(input_tensor)\n",
    "    predicted_index = torch.argmax(prediction).item() # get the index of the maximum log-probability\n",
    "    return index_to_char[predicted_index],inp+index_to_char[predicted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 'those we')\n",
      "('e', ' the')\n",
      "('e', ' ane')\n",
      "('e', 'does the')\n",
      "('n', 'woman')\n",
      "(' ', 'philosoph ')\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_char('those w'))\n",
    "print(predict_next_char(' th'))\n",
    "print(predict_next_char(' an'))\n",
    "print(predict_next_char('does th'))\n",
    "print(predict_next_char('woma'))\n",
    "print(predict_next_char('philosoph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a621ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'simpleTransformer_3pred.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5d4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
