{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350ac8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b77237",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"nietzsche.txt\").read()\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "chars.insert(0, '\\0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92803d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {v:i for i,v in enumerate(chars)}\n",
    "index_to_char = {i:v for i,v in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a63a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_index = [char_to_index[char] for char in text]\n",
    "pred_num = 25 #max character length to input in one go\n",
    "xin = [[total_index[j+i] for j in range(0, len(total_index)-1-pred_num, pred_num)] for i in range(pred_num)]\n",
    "y = [total_index[i+pred_num] for i in range(0, len(total_index)-1-pred_num, pred_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a97901",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.stack(xin[i][:-2]) for i in range(pred_num)],1)\n",
    "Y = np.stack(y[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ca877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa6516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.W_ii = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        \n",
    "        self.W_if = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        \n",
    "        self.W_ig = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        \n",
    "        self.W_io = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.uniform_(param, -0.1, 0.1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        i_t = torch.sigmoid(x @ self.W_ii.T + h_prev @ self.W_hi.T + self.b_i)\n",
    "        f_t = torch.sigmoid(x @ self.W_if.T + h_prev @ self.W_hf.T + self.b_f)\n",
    "        g_t = torch.tanh(x @ self.W_ig.T + h_prev @ self.W_hg.T + self.b_g)\n",
    "        o_t = torch.sigmoid(x @ self.W_io.T + h_prev @ self.W_ho.T + self.b_o)\n",
    "        \n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, (h_t, c_t)\n",
    "\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_layers=1):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_layers = num_layers\n",
    "        self.cells = nn.ModuleList([LSTMCell(input_size, hidden_layers) if i == 0 \n",
    "                                    else LSTMCell(hidden_layers, hidden_layers) \n",
    "                                    for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = [torch.zeros(batch_size, self.hidden_layers).to(x.device) for _ in range(self.num_layers)]\n",
    "        c = [torch.zeros(batch_size, self.hidden_layers).to(x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        out = torch.zeros(batch_size, seq_len, self.hidden_layers, device=x.device) # Initialize output tensor\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h[i], (h[i], c[i]) = cell(x_t, (h[i], c[i]))\n",
    "                x_t = h[i]\n",
    "                out[:,i,:] = x_t\n",
    "        \n",
    "        h_n = x_t\n",
    "        return out,h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "772aec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_layers):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM( input_size=embedding_dim, hidden_size=hidden_layers, batch_first=True )\n",
    "        # self.rnn = CustomLSTM(embedding_dim, hidden_layers)  \n",
    "        self.dense = nn.Linear(hidden_layers, vocab_size)\n",
    "\n",
    "        nn.init.xavier_normal_(self.embedding.weight)\n",
    "        nn.init.xavier_normal_(self.dense.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, _ = self.rnn(embedded)\n",
    "        out = self.dense(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "543c215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (same as Keras)\n",
    "hidden_layers = 128\n",
    "vocab_size = 86\n",
    "embedding_dim = 42\n",
    "batch_size = 64\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61de2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(vocab_size, embedding_dim, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd5b63e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SimpleLSTM                               --\n",
       "├─Embedding: 1-1                         3,612\n",
       "├─LSTM: 1-2                              88,064\n",
       "├─Linear: 1-3                            11,094\n",
       "=================================================================\n",
       "Total params: 102,770\n",
       "Trainable params: 102,770\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "996ddcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # PyTorch uses CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6817938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 4.4554, Time: 1.92s\n",
      "Epoch [2/150], Loss: 4.4396, Time: 1.42s\n",
      "Epoch [3/150], Loss: 4.4233, Time: 1.40s\n",
      "Epoch [4/150], Loss: 4.4059, Time: 1.48s\n",
      "Epoch [5/150], Loss: 4.3869, Time: 1.36s\n",
      "Epoch [6/150], Loss: 4.3657, Time: 1.35s\n",
      "Epoch [7/150], Loss: 4.3412, Time: 1.29s\n",
      "Epoch [8/150], Loss: 4.3119, Time: 1.39s\n",
      "Epoch [9/150], Loss: 4.2755, Time: 1.32s\n",
      "Epoch [10/150], Loss: 4.2283, Time: 1.29s\n",
      "Epoch [11/150], Loss: 4.1636, Time: 1.34s\n",
      "Epoch [12/150], Loss: 4.0696, Time: 1.35s\n",
      "Epoch [13/150], Loss: 3.9266, Time: 1.38s\n",
      "Epoch [14/150], Loss: 3.7217, Time: 1.37s\n",
      "Epoch [15/150], Loss: 3.5346, Time: 1.42s\n",
      "Epoch [16/150], Loss: 3.5306, Time: 1.33s\n",
      "Epoch [17/150], Loss: 3.5316, Time: 1.46s\n",
      "Epoch [18/150], Loss: 3.4801, Time: 1.30s\n",
      "Epoch [19/150], Loss: 3.4088, Time: 1.30s\n",
      "Epoch [20/150], Loss: 3.3394, Time: 1.30s\n",
      "Epoch [21/150], Loss: 3.2828, Time: 1.36s\n",
      "Epoch [22/150], Loss: 3.2420, Time: 1.35s\n",
      "Epoch [23/150], Loss: 3.2153, Time: 1.28s\n",
      "Epoch [24/150], Loss: 3.1989, Time: 1.29s\n",
      "Epoch [25/150], Loss: 3.1891, Time: 1.29s\n",
      "Epoch [26/150], Loss: 3.1828, Time: 1.27s\n",
      "Epoch [27/150], Loss: 3.1782, Time: 1.28s\n",
      "Epoch [28/150], Loss: 3.1742, Time: 1.29s\n",
      "Epoch [29/150], Loss: 3.1705, Time: 1.28s\n",
      "Epoch [30/150], Loss: 3.1667, Time: 1.29s\n",
      "Epoch [31/150], Loss: 3.1629, Time: 1.29s\n",
      "Epoch [32/150], Loss: 3.1589, Time: 1.28s\n",
      "Epoch [33/150], Loss: 3.1547, Time: 1.29s\n",
      "Epoch [34/150], Loss: 3.1501, Time: 1.28s\n",
      "Epoch [35/150], Loss: 3.1450, Time: 1.29s\n",
      "Epoch [36/150], Loss: 3.1396, Time: 1.31s\n",
      "Epoch [37/150], Loss: 3.1341, Time: 1.29s\n",
      "Epoch [38/150], Loss: 3.1286, Time: 1.29s\n",
      "Epoch [39/150], Loss: 3.1233, Time: 1.29s\n",
      "Epoch [40/150], Loss: 3.1186, Time: 1.28s\n",
      "Epoch [41/150], Loss: 3.1144, Time: 1.28s\n",
      "Epoch [42/150], Loss: 3.1110, Time: 1.28s\n",
      "Epoch [43/150], Loss: 3.1082, Time: 1.28s\n",
      "Epoch [44/150], Loss: 3.1060, Time: 1.28s\n",
      "Epoch [45/150], Loss: 3.1044, Time: 1.30s\n",
      "Epoch [46/150], Loss: 3.1032, Time: 1.29s\n",
      "Epoch [47/150], Loss: 3.1022, Time: 1.28s\n",
      "Epoch [48/150], Loss: 3.1014, Time: 1.27s\n",
      "Epoch [49/150], Loss: 3.1006, Time: 1.28s\n",
      "Epoch [50/150], Loss: 3.0996, Time: 1.28s\n",
      "Epoch [51/150], Loss: 3.0984, Time: 1.28s\n",
      "Epoch [52/150], Loss: 3.0970, Time: 1.29s\n",
      "Epoch [53/150], Loss: 3.0954, Time: 1.28s\n",
      "Epoch [54/150], Loss: 3.0936, Time: 1.29s\n",
      "Epoch [55/150], Loss: 3.0917, Time: 1.28s\n",
      "Epoch [56/150], Loss: 3.0897, Time: 1.28s\n",
      "Epoch [57/150], Loss: 3.0878, Time: 1.28s\n",
      "Epoch [58/150], Loss: 3.0859, Time: 1.28s\n",
      "Epoch [59/150], Loss: 3.0842, Time: 1.26s\n",
      "Epoch [60/150], Loss: 3.0824, Time: 1.29s\n",
      "Epoch [61/150], Loss: 3.0808, Time: 1.27s\n",
      "Epoch [62/150], Loss: 3.0792, Time: 1.29s\n",
      "Epoch [63/150], Loss: 3.0776, Time: 1.30s\n",
      "Epoch [64/150], Loss: 3.0759, Time: 1.29s\n",
      "Epoch [65/150], Loss: 3.0742, Time: 1.28s\n",
      "Epoch [66/150], Loss: 3.0725, Time: 1.28s\n",
      "Epoch [67/150], Loss: 3.0706, Time: 1.28s\n",
      "Epoch [68/150], Loss: 3.0687, Time: 1.29s\n",
      "Epoch [69/150], Loss: 3.0666, Time: 1.27s\n",
      "Epoch [70/150], Loss: 3.0645, Time: 1.29s\n",
      "Epoch [71/150], Loss: 3.0623, Time: 1.28s\n",
      "Epoch [72/150], Loss: 3.0600, Time: 1.29s\n",
      "Epoch [73/150], Loss: 3.0576, Time: 1.28s\n",
      "Epoch [74/150], Loss: 3.0551, Time: 1.29s\n",
      "Epoch [75/150], Loss: 3.0526, Time: 1.28s\n",
      "Epoch [76/150], Loss: 3.0500, Time: 1.27s\n",
      "Epoch [77/150], Loss: 3.0473, Time: 1.29s\n",
      "Epoch [78/150], Loss: 3.0445, Time: 1.28s\n",
      "Epoch [79/150], Loss: 3.0417, Time: 1.29s\n",
      "Epoch [80/150], Loss: 3.0387, Time: 1.29s\n",
      "Epoch [81/150], Loss: 3.0356, Time: 1.29s\n",
      "Epoch [82/150], Loss: 3.0324, Time: 1.27s\n",
      "Epoch [83/150], Loss: 3.0291, Time: 1.29s\n",
      "Epoch [84/150], Loss: 3.0256, Time: 1.29s\n",
      "Epoch [85/150], Loss: 3.0221, Time: 1.29s\n",
      "Epoch [86/150], Loss: 3.0184, Time: 1.30s\n",
      "Epoch [87/150], Loss: 3.0145, Time: 1.28s\n",
      "Epoch [88/150], Loss: 3.0106, Time: 1.30s\n",
      "Epoch [89/150], Loss: 3.0065, Time: 1.28s\n",
      "Epoch [90/150], Loss: 3.0022, Time: 1.27s\n",
      "Epoch [91/150], Loss: 2.9979, Time: 1.29s\n",
      "Epoch [92/150], Loss: 2.9934, Time: 1.27s\n",
      "Epoch [93/150], Loss: 2.9888, Time: 1.28s\n",
      "Epoch [94/150], Loss: 2.9840, Time: 1.30s\n",
      "Epoch [95/150], Loss: 2.9791, Time: 1.28s\n",
      "Epoch [96/150], Loss: 2.9740, Time: 1.28s\n",
      "Epoch [97/150], Loss: 2.9688, Time: 1.28s\n",
      "Epoch [98/150], Loss: 2.9634, Time: 1.28s\n",
      "Epoch [99/150], Loss: 2.9579, Time: 1.29s\n",
      "Epoch [100/150], Loss: 2.9523, Time: 1.29s\n",
      "Epoch [101/150], Loss: 2.9465, Time: 1.28s\n",
      "Epoch [102/150], Loss: 2.9407, Time: 1.29s\n",
      "Epoch [103/150], Loss: 2.9347, Time: 1.28s\n",
      "Epoch [104/150], Loss: 2.9286, Time: 1.28s\n",
      "Epoch [105/150], Loss: 2.9224, Time: 1.29s\n",
      "Epoch [106/150], Loss: 2.9161, Time: 1.28s\n",
      "Epoch [107/150], Loss: 2.9097, Time: 1.29s\n",
      "Epoch [108/150], Loss: 2.9032, Time: 1.27s\n",
      "Epoch [109/150], Loss: 2.8967, Time: 1.28s\n",
      "Epoch [110/150], Loss: 2.8900, Time: 1.29s\n",
      "Epoch [111/150], Loss: 2.8833, Time: 1.27s\n",
      "Epoch [112/150], Loss: 2.8766, Time: 1.28s\n",
      "Epoch [113/150], Loss: 2.8698, Time: 1.28s\n",
      "Epoch [114/150], Loss: 2.8629, Time: 1.28s\n",
      "Epoch [115/150], Loss: 2.8560, Time: 1.28s\n",
      "Epoch [116/150], Loss: 2.8491, Time: 1.30s\n",
      "Epoch [117/150], Loss: 2.8422, Time: 1.28s\n",
      "Epoch [118/150], Loss: 2.8353, Time: 1.28s\n",
      "Epoch [119/150], Loss: 2.8284, Time: 1.28s\n",
      "Epoch [120/150], Loss: 2.8216, Time: 1.29s\n",
      "Epoch [121/150], Loss: 2.8147, Time: 1.30s\n",
      "Epoch [122/150], Loss: 2.8079, Time: 1.35s\n",
      "Epoch [123/150], Loss: 2.8011, Time: 1.82s\n",
      "Epoch [124/150], Loss: 2.7944, Time: 1.31s\n",
      "Epoch [125/150], Loss: 2.7877, Time: 1.30s\n",
      "Epoch [126/150], Loss: 2.7810, Time: 1.29s\n",
      "Epoch [127/150], Loss: 2.7744, Time: 1.34s\n",
      "Epoch [128/150], Loss: 2.7679, Time: 1.48s\n",
      "Epoch [129/150], Loss: 2.7614, Time: 1.40s\n",
      "Epoch [130/150], Loss: 2.7550, Time: 1.33s\n",
      "Epoch [131/150], Loss: 2.7487, Time: 1.36s\n",
      "Epoch [132/150], Loss: 2.7425, Time: 1.42s\n",
      "Epoch [133/150], Loss: 2.7364, Time: 1.42s\n",
      "Epoch [134/150], Loss: 2.7303, Time: 1.38s\n",
      "Epoch [135/150], Loss: 2.7244, Time: 1.38s\n",
      "Epoch [136/150], Loss: 2.7186, Time: 1.30s\n",
      "Epoch [137/150], Loss: 2.7128, Time: 1.46s\n",
      "Epoch [138/150], Loss: 2.7071, Time: 1.31s\n",
      "Epoch [139/150], Loss: 2.7015, Time: 1.48s\n",
      "Epoch [140/150], Loss: 2.6960, Time: 1.30s\n",
      "Epoch [141/150], Loss: 2.6905, Time: 1.29s\n",
      "Epoch [142/150], Loss: 2.6851, Time: 1.28s\n",
      "Epoch [143/150], Loss: 2.6798, Time: 1.30s\n",
      "Epoch [144/150], Loss: 2.6746, Time: 1.30s\n",
      "Epoch [145/150], Loss: 2.6695, Time: 1.32s\n",
      "Epoch [146/150], Loss: 2.6644, Time: 1.48s\n",
      "Epoch [147/150], Loss: 2.6595, Time: 1.40s\n",
      "Epoch [148/150], Loss: 2.6546, Time: 1.29s\n",
      "Epoch [149/150], Loss: 2.6498, Time: 1.29s\n",
      "Epoch [150/150], Loss: 2.6451, Time: 1.28s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    current_time = time.time()\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad() # zero the gradient\n",
    "    outputs = model(X_tensor) # foward pass\n",
    "    loss = criterion(outputs, Y_tensor) # calculate the loss\n",
    "    loss.backward() # calculate the gradients\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Time: {time.time() - current_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "651e953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(inp):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    index = [char_to_index[i] for i in inp]\n",
    "    arr = np.expand_dims(np.array(index), axis=0)\n",
    "    input_tensor = torch.tensor(arr, dtype=torch.long) # Convert to tensor\n",
    "    with torch.no_grad(): # Disable gradient calculation during inference\n",
    "        prediction = model(input_tensor)\n",
    "    predicted_index = torch.argmax(prediction).item() # get the index of the maximum log-probability\n",
    "    return index_to_char[predicted_index],inp+index_to_char[predicted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef073df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('h', 'those wh')\n",
      "('e', ' the')\n",
      "('d', ' and')\n",
      "('e', 'does the')\n",
      "('n', 'woman')\n",
      "('e', 'philosophe')\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_char('those w'))\n",
    "print(predict_next_char(' th'))\n",
    "print(predict_next_char(' an'))\n",
    "print(predict_next_char('does th'))\n",
    "print(predict_next_char('woma'))\n",
    "print(predict_next_char('philosoph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9d6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f1966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
