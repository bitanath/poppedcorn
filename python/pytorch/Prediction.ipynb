{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "007f6c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cbfd8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../src/server/movies.json\") as f:\n",
    "    all_movies = json.load(f)\n",
    "    movies = [movie[\"name\"] for movie in all_movies]\n",
    "    dataset = []\n",
    "    padding = '\\0'\n",
    "    char_length = 10\n",
    "    for item in movies:\n",
    "        movie = ''.join(char for char in item.lower() if char.isalpha())\n",
    "        \n",
    "        if len(movie) < 2:\n",
    "            continue\n",
    "        \n",
    "        for i in range(1, len(movie)):\n",
    "            prev_chars = movie[max(0, i-char_length):i]\n",
    "            if len(prev_chars) < char_length:\n",
    "                prev_chars = padding * (char_length - len(prev_chars)) + prev_chars\n",
    "            \n",
    "            current_char = movie[i]\n",
    "            \n",
    "            dataset.append((prev_chars, current_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a6d08982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00f', 'a'),\n",
       " ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00fa', 'n'),\n",
       " ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00fan', 't'),\n",
       " ('\\x00\\x00\\x00\\x00\\x00\\x00fant', 'a'),\n",
       " ('\\x00\\x00\\x00\\x00\\x00fanta', 's'),\n",
       " ('\\x00\\x00\\x00\\x00fantas', 't'),\n",
       " ('\\x00\\x00\\x00fantast', 'i'),\n",
       " ('\\x00\\x00fantasti', 'c'),\n",
       " ('\\x00fantastic', 'f'),\n",
       " ('fantasticf', 'o')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "493eb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "chars.insert(0, '\\0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "be6a4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {v:i for i,v in enumerate(chars)}\n",
    "index_to_char = {i:v for i,v in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ad96632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xin = np.array([[char_to_index[ch] for ch in char[0]] for char in dataset]).T\n",
    "y = [char_to_index[char[1]] for char in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ac9fb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_num = char_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "34854c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.stack(xin[i][:-2]) for i in range(pred_num)],1)\n",
    "Y = np.stack(y[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7e0cfff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12962, 10) (12962,) ['f', 'a', 'n', 't', 'a', 's', 't', 'i', 'c', 'f'] ['o']\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,Y.shape,[index_to_char[x] for x in X[9]],[index_to_char[Y[9]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3f4710a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27c98c",
   "metadata": {},
   "source": [
    "#### Model architecture experimented and iterated with in Pytorch * notebooks:\n",
    "Sticking with RNN since its a simple model with high accuracy and easy deployability (weights come in at about 200kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "08734c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nonlinearity='tanh'):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_ih = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_hh = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.W_ih, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_hh, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_ih)\n",
    "        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "        nn.init.uniform_(self.b_ih, -bound, bound)\n",
    "        nn.init.uniform_(self.b_hh, -bound, bound)\n",
    "\n",
    "\n",
    "    def forward(self, x, h_0=None):\n",
    "        batch_size, seq_len, input_size = x.size()\n",
    "        hidden_size = self.hidden_size\n",
    "        if h_0 is None:\n",
    "            h_t = torch.zeros(batch_size, hidden_size, device=x.device) # Initialize hidden state if not provided\n",
    "        else:\n",
    "            h_t = h_0\n",
    "        output = torch.zeros(batch_size, seq_len, hidden_size, device=x.device) # Initialize output tensor\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            h_t = torch.tanh(F.linear(x_t, self.W_ih, self.b_ih) + F.linear(h_t, self.W_hh, self.b_hh)) #TODO: tanh is hardcoded as activation rn\n",
    "            output[:, t, :] = h_t\n",
    "        h_n = h_t\n",
    "\n",
    "        return output, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9d22bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = RecurrentLayer(embedding_dim, hidden_layers, nonlinearity='tanh')  \n",
    "        self.dense = nn.Linear(hidden_layers, vocab_size)\n",
    "        nn.init.xavier_normal_(self.embedding.weight)\n",
    "        nn.init.xavier_normal_(self.dense.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, _ = self.rnn(embedded)\n",
    "        out = self.dense(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0fcffb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_layers = 168 #ideal = 320, practical = 168\n",
    "vocab_size = len(chars)\n",
    "embedding_dim = 84 #ideal = 256,practical = 84\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7902b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_layers)\n",
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "51754514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 49503 Total: 49503\n",
      "Approx. Model size: 0.57MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable Parameters:\",sum(p.numel() for p in model.parameters() if p.requires_grad),\"Total:\",sum(p.numel() for p in model.parameters()))\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('Approx. Model size: {:.2f}MB'.format(3*size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bbe05ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "pbar = tqdm.tqdm(range(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7d10d8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.81it/s, loss=0.8550, time/epoch=0.24s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in pbar:\n",
    "    current_time = time.time()\n",
    "    model.train() \n",
    "    optimizer.zero_grad() \n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, Y_tensor) \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "    pbar.set_postfix({\"loss\":f\"{loss.item():.4f}\",\"time/epoch\":f\"{time.time() - current_time:.2f}s\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c0c34732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(inp):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    index = [char_to_index[i] for i in inp]\n",
    "    arr = np.expand_dims(np.array(index), axis=0)\n",
    "    input_tensor = torch.tensor(arr, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "    predicted_index = torch.argmax(prediction).item() # get the index of the maximum log-probability\n",
    "    return index_to_char[predicted_index],inp+index_to_char[predicted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "55eb7d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r', 'star')\n",
      "('i', 'thematri')\n",
      "('d', 'asgood')\n",
      "('a', 'thegodfathea')\n",
      "('n', 'un')\n",
      "('n', 'aquaman')\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_char('sta'))\n",
    "print(predict_next_char('thematr'))\n",
    "print(predict_next_char('asgoo'))\n",
    "print(predict_next_char('thegodfathe'))\n",
    "print(predict_next_char('u'))\n",
    "print(predict_next_char('aquama'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4d988f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8a790072",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"weights.pth\") and load_pretrained:\n",
    "    model.load_state_dict(torch.load(\"weights.pth\"))\n",
    "    state_dict = model.state_dict()\n",
    "else:\n",
    "    state_dict = model.state_dict()\n",
    "    torch.save(state_dict,\"weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6b0554ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_json = OrderedDict({key: state_dict[key].detach().cpu().half().numpy() for key in state_dict})\n",
    "for key,value in weights_json.items():\n",
    "    weights_json[key] = value.astype('float').round(3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b5a7d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weights.json\",\"w\") as f:\n",
    "    json.dump(weights_json,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "387c21fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx. JSON size: 0.35MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Approx. JSON size: {:.2f}MB\".format(os.path.getsize(\"weights.json\")/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3921db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
