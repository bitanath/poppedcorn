{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038f79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be558398",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"nietzsche.txt\").read()\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "chars.insert(0, '\\0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100b798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {v:i for i,v in enumerate(chars)}\n",
    "index_to_char = {i:v for i,v in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d777733",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_index = [char_to_index[char] for char in text]\n",
    "pred_num = 25 #max character length to input in one go\n",
    "xin = [[total_index[j+i] for j in range(0, len(total_index)-1-pred_num, pred_num)] for i in range(pred_num)]\n",
    "y = [total_index[i+pred_num] for i in range(0, len(total_index)-1-pred_num, pred_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfe0d2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24035,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d567d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.stack(xin[i][:-2]) for i in range(pred_num)],1)\n",
    "Y = np.stack(y[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c08aef2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P',\n",
       " 'R',\n",
       " 'E',\n",
       " 'F',\n",
       " 'A',\n",
       " 'C',\n",
       " 'E',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'S',\n",
       " 'U',\n",
       " 'P',\n",
       " 'P',\n",
       " 'O',\n",
       " 'S',\n",
       " 'I',\n",
       " 'N',\n",
       " 'G',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index_to_char[x] for x in X[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3caf8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfd4861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nonlinearity='tanh'):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_ih = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_hh = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.W_ih, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_hh, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_ih)\n",
    "        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "        nn.init.uniform_(self.b_ih, -bound, bound)\n",
    "        nn.init.uniform_(self.b_hh, -bound, bound)\n",
    "\n",
    "\n",
    "    def forward(self, x, h_0=None):\n",
    "        batch_size, seq_len, input_size = x.size()\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        if h_0 is None:\n",
    "            h_t = torch.zeros(batch_size, hidden_size, device=x.device) # Initialize hidden state if not provided\n",
    "        else:\n",
    "            h_t = h_0\n",
    "\n",
    "        output = torch.zeros(batch_size, seq_len, hidden_size, device=x.device) # Initialize output tensor\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]  # Input at time t (batch_size, input_size)\n",
    "\n",
    "            \n",
    "            h_t = torch.tanh(F.linear(x_t, self.W_ih, self.b_ih) + F.linear(h_t, self.W_hh, self.b_hh)) #TODO: tanh is hardcoded as activation rn\n",
    "            output[:, t, :] = h_t\n",
    "\n",
    "        h_n = h_t # h_n is the last hidden state\n",
    "\n",
    "        return output, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1fdb5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # self.rnn = nn.RNN(embedding_dim, hidden_layers, nonlinearity='tanh', batch_first=True) \n",
    "        self.rnn = CustomRNN(embedding_dim, hidden_layers, nonlinearity='tanh')  \n",
    "        self.dense = nn.Linear(hidden_layers, vocab_size)\n",
    "\n",
    "        nn.init.xavier_normal_(self.embedding.weight)\n",
    "        nn.init.xavier_normal_(self.dense.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, _ = self.rnn(embedded)\n",
    "        out = self.dense(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47962576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (same as Keras)\n",
    "hidden_layers = 128\n",
    "vocab_size = 86\n",
    "embedding_dim = 42\n",
    "batch_size = 64\n",
    "epochs = 150 #150 works well for data with 24033 rows, adjust accordingly for data with fewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc42f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24033, 25)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Trainable Parameters:\",sum(p.numel() for p in model.parameters() if p.requires_grad),\"Total:\",sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa8a5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d7cf163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SimpleRNN                                --\n",
       "├─Embedding: 1-1                         3,612\n",
       "├─CustomRNN: 1-2                         22,016\n",
       "├─Linear: 1-3                            11,094\n",
       "=================================================================\n",
       "Total params: 36,722\n",
       "Trainable params: 36,722\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e6a4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # PyTorch uses CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44e0addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 4.4605, Time: 1.27s\n",
      "Epoch [2/150], Loss: 4.3909, Time: 1.16s\n",
      "Epoch [3/150], Loss: 4.3233, Time: 1.14s\n",
      "Epoch [4/150], Loss: 4.2529, Time: 1.27s\n",
      "Epoch [5/150], Loss: 4.1746, Time: 1.23s\n",
      "Epoch [6/150], Loss: 4.0829, Time: 1.14s\n",
      "Epoch [7/150], Loss: 3.9721, Time: 1.17s\n",
      "Epoch [8/150], Loss: 3.8412, Time: 1.18s\n",
      "Epoch [9/150], Loss: 3.7024, Time: 1.14s\n",
      "Epoch [10/150], Loss: 3.5789, Time: 1.13s\n",
      "Epoch [11/150], Loss: 3.4857, Time: 1.12s\n",
      "Epoch [12/150], Loss: 3.4183, Time: 1.12s\n",
      "Epoch [13/150], Loss: 3.3627, Time: 1.13s\n",
      "Epoch [14/150], Loss: 3.3117, Time: 1.13s\n",
      "Epoch [15/150], Loss: 3.2660, Time: 1.13s\n",
      "Epoch [16/150], Loss: 3.2276, Time: 1.13s\n",
      "Epoch [17/150], Loss: 3.1972, Time: 1.13s\n",
      "Epoch [18/150], Loss: 3.1744, Time: 1.12s\n",
      "Epoch [19/150], Loss: 3.1587, Time: 1.12s\n",
      "Epoch [20/150], Loss: 3.1486, Time: 1.11s\n",
      "Epoch [21/150], Loss: 3.1422, Time: 1.13s\n",
      "Epoch [22/150], Loss: 3.1373, Time: 1.13s\n",
      "Epoch [23/150], Loss: 3.1325, Time: 1.13s\n",
      "Epoch [24/150], Loss: 3.1269, Time: 1.13s\n",
      "Epoch [25/150], Loss: 3.1209, Time: 1.14s\n",
      "Epoch [26/150], Loss: 3.1148, Time: 1.12s\n",
      "Epoch [27/150], Loss: 3.1094, Time: 1.18s\n",
      "Epoch [28/150], Loss: 3.1052, Time: 1.18s\n",
      "Epoch [29/150], Loss: 3.1023, Time: 1.12s\n",
      "Epoch [30/150], Loss: 3.1003, Time: 1.13s\n",
      "Epoch [31/150], Loss: 3.0984, Time: 1.10s\n",
      "Epoch [32/150], Loss: 3.0958, Time: 1.12s\n",
      "Epoch [33/150], Loss: 3.0924, Time: 1.15s\n",
      "Epoch [34/150], Loss: 3.0883, Time: 1.13s\n",
      "Epoch [35/150], Loss: 3.0842, Time: 1.14s\n",
      "Epoch [36/150], Loss: 3.0803, Time: 1.13s\n",
      "Epoch [37/150], Loss: 3.0771, Time: 1.14s\n",
      "Epoch [38/150], Loss: 3.0742, Time: 1.12s\n",
      "Epoch [39/150], Loss: 3.0715, Time: 1.15s\n",
      "Epoch [40/150], Loss: 3.0685, Time: 1.16s\n",
      "Epoch [41/150], Loss: 3.0651, Time: 1.12s\n",
      "Epoch [42/150], Loss: 3.0613, Time: 1.13s\n",
      "Epoch [43/150], Loss: 3.0572, Time: 1.12s\n",
      "Epoch [44/150], Loss: 3.0530, Time: 1.12s\n",
      "Epoch [45/150], Loss: 3.0489, Time: 1.13s\n",
      "Epoch [46/150], Loss: 3.0448, Time: 1.13s\n",
      "Epoch [47/150], Loss: 3.0408, Time: 1.14s\n",
      "Epoch [48/150], Loss: 3.0366, Time: 1.22s\n",
      "Epoch [49/150], Loss: 3.0324, Time: 1.16s\n",
      "Epoch [50/150], Loss: 3.0279, Time: 1.12s\n",
      "Epoch [51/150], Loss: 3.0232, Time: 1.14s\n",
      "Epoch [52/150], Loss: 3.0184, Time: 1.16s\n",
      "Epoch [53/150], Loss: 3.0135, Time: 1.11s\n",
      "Epoch [54/150], Loss: 3.0085, Time: 1.13s\n",
      "Epoch [55/150], Loss: 3.0034, Time: 1.12s\n",
      "Epoch [56/150], Loss: 2.9980, Time: 1.13s\n",
      "Epoch [57/150], Loss: 2.9926, Time: 1.12s\n",
      "Epoch [58/150], Loss: 2.9869, Time: 1.13s\n",
      "Epoch [59/150], Loss: 2.9811, Time: 1.13s\n",
      "Epoch [60/150], Loss: 2.9751, Time: 1.12s\n",
      "Epoch [61/150], Loss: 2.9691, Time: 1.13s\n",
      "Epoch [62/150], Loss: 2.9629, Time: 1.13s\n",
      "Epoch [63/150], Loss: 2.9565, Time: 1.13s\n",
      "Epoch [64/150], Loss: 2.9500, Time: 1.13s\n",
      "Epoch [65/150], Loss: 2.9433, Time: 1.15s\n",
      "Epoch [66/150], Loss: 2.9365, Time: 1.13s\n",
      "Epoch [67/150], Loss: 2.9296, Time: 1.14s\n",
      "Epoch [68/150], Loss: 2.9227, Time: 1.12s\n",
      "Epoch [69/150], Loss: 2.9157, Time: 1.11s\n",
      "Epoch [70/150], Loss: 2.9087, Time: 1.14s\n",
      "Epoch [71/150], Loss: 2.9017, Time: 1.13s\n",
      "Epoch [72/150], Loss: 2.8947, Time: 1.11s\n",
      "Epoch [73/150], Loss: 2.8877, Time: 1.16s\n",
      "Epoch [74/150], Loss: 2.8807, Time: 1.13s\n",
      "Epoch [75/150], Loss: 2.8737, Time: 1.13s\n",
      "Epoch [76/150], Loss: 2.8668, Time: 1.14s\n",
      "Epoch [77/150], Loss: 2.8599, Time: 1.13s\n",
      "Epoch [78/150], Loss: 2.8530, Time: 1.15s\n",
      "Epoch [79/150], Loss: 2.8461, Time: 1.12s\n",
      "Epoch [80/150], Loss: 2.8393, Time: 1.12s\n",
      "Epoch [81/150], Loss: 2.8325, Time: 1.14s\n",
      "Epoch [82/150], Loss: 2.8257, Time: 1.12s\n",
      "Epoch [83/150], Loss: 2.8190, Time: 1.13s\n",
      "Epoch [84/150], Loss: 2.8122, Time: 1.13s\n",
      "Epoch [85/150], Loss: 2.8055, Time: 1.13s\n",
      "Epoch [86/150], Loss: 2.7988, Time: 1.13s\n",
      "Epoch [87/150], Loss: 2.7921, Time: 1.12s\n",
      "Epoch [88/150], Loss: 2.7854, Time: 1.13s\n",
      "Epoch [89/150], Loss: 2.7788, Time: 1.13s\n",
      "Epoch [90/150], Loss: 2.7722, Time: 1.13s\n",
      "Epoch [91/150], Loss: 2.7656, Time: 1.18s\n",
      "Epoch [92/150], Loss: 2.7591, Time: 1.13s\n",
      "Epoch [93/150], Loss: 2.7526, Time: 1.13s\n",
      "Epoch [94/150], Loss: 2.7461, Time: 1.13s\n",
      "Epoch [95/150], Loss: 2.7397, Time: 1.12s\n",
      "Epoch [96/150], Loss: 2.7334, Time: 1.16s\n",
      "Epoch [97/150], Loss: 2.7271, Time: 1.12s\n",
      "Epoch [98/150], Loss: 2.7208, Time: 1.14s\n",
      "Epoch [99/150], Loss: 2.7148, Time: 1.14s\n",
      "Epoch [100/150], Loss: 2.7086, Time: 1.13s\n",
      "Epoch [101/150], Loss: 2.7024, Time: 1.12s\n",
      "Epoch [102/150], Loss: 2.6964, Time: 1.13s\n",
      "Epoch [103/150], Loss: 2.6904, Time: 1.15s\n",
      "Epoch [104/150], Loss: 2.6845, Time: 1.12s\n",
      "Epoch [105/150], Loss: 2.6787, Time: 1.13s\n",
      "Epoch [106/150], Loss: 2.6729, Time: 1.11s\n",
      "Epoch [107/150], Loss: 2.6673, Time: 1.13s\n",
      "Epoch [108/150], Loss: 2.6617, Time: 1.11s\n",
      "Epoch [109/150], Loss: 2.6561, Time: 1.14s\n",
      "Epoch [110/150], Loss: 2.6507, Time: 1.11s\n",
      "Epoch [111/150], Loss: 2.6453, Time: 1.13s\n",
      "Epoch [112/150], Loss: 2.6399, Time: 1.13s\n",
      "Epoch [113/150], Loss: 2.6346, Time: 1.12s\n",
      "Epoch [114/150], Loss: 2.6294, Time: 1.13s\n",
      "Epoch [115/150], Loss: 2.6242, Time: 1.12s\n",
      "Epoch [116/150], Loss: 2.6191, Time: 1.14s\n",
      "Epoch [117/150], Loss: 2.6140, Time: 1.14s\n",
      "Epoch [118/150], Loss: 2.6090, Time: 1.13s\n",
      "Epoch [119/150], Loss: 2.6039, Time: 1.16s\n",
      "Epoch [120/150], Loss: 2.5989, Time: 1.12s\n",
      "Epoch [121/150], Loss: 2.5939, Time: 1.12s\n",
      "Epoch [122/150], Loss: 2.5889, Time: 1.12s\n",
      "Epoch [123/150], Loss: 2.5839, Time: 1.13s\n",
      "Epoch [124/150], Loss: 2.5791, Time: 1.12s\n",
      "Epoch [125/150], Loss: 2.5746, Time: 1.12s\n",
      "Epoch [126/150], Loss: 2.5703, Time: 1.16s\n",
      "Epoch [127/150], Loss: 2.5661, Time: 1.12s\n",
      "Epoch [128/150], Loss: 2.5618, Time: 1.13s\n",
      "Epoch [129/150], Loss: 2.5573, Time: 1.14s\n",
      "Epoch [130/150], Loss: 2.5529, Time: 1.12s\n",
      "Epoch [131/150], Loss: 2.5487, Time: 1.13s\n",
      "Epoch [132/150], Loss: 2.5447, Time: 1.13s\n",
      "Epoch [133/150], Loss: 2.5408, Time: 1.13s\n",
      "Epoch [134/150], Loss: 2.5369, Time: 1.12s\n",
      "Epoch [135/150], Loss: 2.5329, Time: 1.13s\n",
      "Epoch [136/150], Loss: 2.5290, Time: 1.13s\n",
      "Epoch [137/150], Loss: 2.5252, Time: 1.13s\n",
      "Epoch [138/150], Loss: 2.5214, Time: 1.12s\n",
      "Epoch [139/150], Loss: 2.5178, Time: 1.12s\n",
      "Epoch [140/150], Loss: 2.5142, Time: 1.11s\n",
      "Epoch [141/150], Loss: 2.5107, Time: 1.14s\n",
      "Epoch [142/150], Loss: 2.5071, Time: 1.14s\n",
      "Epoch [143/150], Loss: 2.5036, Time: 1.12s\n",
      "Epoch [144/150], Loss: 2.5001, Time: 1.15s\n",
      "Epoch [145/150], Loss: 2.4968, Time: 1.13s\n",
      "Epoch [146/150], Loss: 2.4934, Time: 1.12s\n",
      "Epoch [147/150], Loss: 2.4901, Time: 1.15s\n",
      "Epoch [148/150], Loss: 2.4868, Time: 1.13s\n",
      "Epoch [149/150], Loss: 2.4835, Time: 1.14s\n",
      "Epoch [150/150], Loss: 2.4802, Time: 1.13s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    current_time = time.time()\n",
    "    model.train() \n",
    "    optimizer.zero_grad() \n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, Y_tensor) \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Time: {time.time() - current_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9b28a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(inp):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    index = [char_to_index[i] for i in inp]\n",
    "    arr = np.expand_dims(np.array(index), axis=0)\n",
    "    input_tensor = torch.tensor(arr, dtype=torch.long) # Convert to tensor\n",
    "    with torch.no_grad(): # Disable gradient calculation during inference\n",
    "        prediction = model(input_tensor)\n",
    "    predicted_index = torch.argmax(prediction).item() # get the index of the maximum log-probability\n",
    "    return index_to_char[predicted_index],inp+index_to_char[predicted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e22c569f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('h', 'those wh')\n",
      "('e', ' the')\n",
      "('d', ' and')\n",
      "('e', 'does the')\n",
      "('n', 'woman')\n",
      "('e', 'philosophe')\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_char('those w'))\n",
    "print(predict_next_char(' th'))\n",
    "print(predict_next_char(' an'))\n",
    "print(predict_next_char('does th'))\n",
    "print(predict_next_char('woma'))\n",
    "print(predict_next_char('philosoph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9395aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'simpleRNN_3pred.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e896b06",
   "metadata": {},
   "source": [
    "#### Now to load movies and prepare the dataset and character functions to predict next letter of movie name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1660e191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLook in the notebook Prediction.ipynb for model and training code\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Look in the notebook Prediction.ipynb for model and training code\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
